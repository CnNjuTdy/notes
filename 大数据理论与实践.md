适当分流，会和高老师商量好之后告诉大家。

荣老师主要做后勤工作，实验安排，资源分配等等，课程内容主要由企业老师（范老师）来主讲。

课程时间需要用到硬件资源，在课程实验期间（第6周到第11周），云资源会暂时回收，以用作实践。

这门课是一门导论性质的课程，系统地，全面地，概要地讲解大数据的相关概念。

### 课程目标

> 请务必夯实基础

- 开源：掌握大数据技术的基础理论。文件系统，资源管理框架，分布式，计算框架，ETL工具，数仓和SQL引擎，NOSQL数据库，全文搜索引擎
- 工业届：掌握星环TDH平台的基本原理和使用方法。数仓和SQL引擎，流处理引擎，NewSQL数据库，全文搜索引擎

### 课程安排

> 要宏观了解，搞清楚整体走向

第1周 技术概况

第2周 Hadoop生态圈-HDFS(文件系统)

第3周 Hadoop生态圈-YARN(资源管理系统)

第4周 Hadoop生态圈-SPARK&MR(计算框架)

第5周 Hadoop生态圈-分布式ETL工具

第6周 第一次实验：装载Hadoop集群

第7周 大数据平台组件-Incpter（数仓）

第8周 大数据平台组件-Slipstream（流处理引擎）

第9周 大数据平台组件-HyperBase（NoSQL）

第10周 大数据平台组件-Search（搜索引擎）&&总结与展望

第11周 第二次实验：应用场景

考试：开卷考试

### 大数据技术综述

> 宏观概念，每个技术在总体是什么概念，属于什么层级，解决从哪入手的问题

大数据技术从宏观来看，不止Hadoop一条技术路线，还有一条技术路线是MPP，这两条技术路线在市场上是并行的。Hadoop解决了海量非结构化或非关系型的数据处理问题。MPP是一个分布式关系数据库技术，它有多个独立的，不做关系共享的关系数据库实例，并组成一个集群，当有一条新的数据或者新的计算结果时，会通过hash映射到随机一个关系数据库，MPP局限性包括只能有关系型数据库，并且集群规模是有上限的，适合中小规模的大数据处理（原本银行，证券交易所会使用MPP，现在正在逐渐迁移到Hadoop）。这两者从接口上正在想办法整合。

本次课程将的是Hadoop技术路线，现在数据的新的特点（特征）催生了Hadoop技术的产生。

大数据是指超出传统数据库工具收集，存储，管理和分析能力的数据集。其主要特征包括

1. 数据量巨大（Volume）：PB以上数量级
2. 数据类型多样（Variety）：网页，文本，日志等有一定结构但是没有关系的数据（半结构化数据）
3. 生成和处理速度极快（Velocity）：移动端的计算设备使得生成数据的速度极快
4. 价值巨大但是密度比较低（Value）：允许有杂音，允许有垃圾，但是有些数据是有价值的，这些价值的密度很低，因此要处理，但是处理难度比较大，需要新的技术

在以后介绍技术的时候，会说明技术是用在什么样的数据，什么样的场景上的

大数据的应用场景

1. 数据仓库：存储海量数据，以提供有价值的决策。

- 传统数仓由业务系统产生，进入传统关系型数据库，然后使用SQL语句进行查询和分析
- 大数据数仓由日志系统，网页爬虫，文本数据提供数据，这些数据不是关系型的数据，不能直接进入关系型数据库。Hadoop全用全栈形式的分布式系统，数据先进入分布式汇聚，再进入分布式菲关系型数据库，（可以添加一个分布式的cube层，用作中间变量）再进行分析

2. 实时流处理：拿到新的消息之后，进入分布式消息队列（卡夫卡：吞吐量单节点1s10w条消息），流处理引擎会从卡夫卡中取数据（毫秒级的事件驱动和分钟级的批处理）并作出一些数据挖掘和分析（决策树），并保存数据，返回一定的消息（异常，告警等）。

#### 大数据技术概览

前期（从无到有）

- 谷歌发布的大数据三篇论文
  - 03年 Google File System （文件系统）
  - 04年 Map Reduce（计算框架）
  - 06年 Big table（半结构数据库）
- 07年雅虎正式使用Hadoop

中期（从有到好）

- 08年 Hadoop称为Apche顶级项目
- 08年 第一个Hadoop商业公司成立并推出第一个发行版CDH
- 12年 HDFS NameNode HA加入Hadoop主版本（稳定性，安全，走向实用）

后期（推广）

- 星环科技发布了国内首个Spark和Hadoop2.0的大数据基础平台软件——TDH

#### 大数据技术体系

> 感觉有点像个计算机

- 数据采集：Snoop（关系型数据导入导出）Flume+Kafka（半结构化，非关系型数据，分布式消息队列）
- 数据存储： HDFS（分布式文件系统）HBase/Cassandra/Hyperbase/MongoDB/Redis/Neo4j（分布式NoSQL数据库系统）
- 资源管理：YARN/Mesos（资源管理框架）Kubernetes/TOS（数据中心操作系统）
- 通用计算：Map-Reduce（批处理计算）Spark Core（高性能计算核心框架）
- 数据分析：Hive/Inceptor（SQL引擎），Search（搜索引擎），Holodesk（OLAP），Spark MLlib（机器学习），Slipstream（流处理引擎）

> NoSQL刚开始就是不支持sql的（no-sql），后来通过变通的方式支持了SQL(not-only-sql)
>
> HDFS（Hadoop Distributed File System）在开源大数据技术体系中，地位无可替代：高容错，高可用，高扩展
>
> 分治思想是大数据里面最重要的思想。资源管理层可以类比分配师，分配钢筋水泥，通用计算层可以类比建筑图纸，指导建筑过程。Map-Reduce比较粗糙，Spark Core比较精致
>
> Kubernetes以容器为单位做资源调度和任务分配
>
> Spark项目包括应用层的Spark SQL，Spark MLlib，Spark Streaming，Spark GraphX以及通用计算层的Spark Core
>
> YARN：Yet Another Resource Negotiator，另一种资源管理器：专注于资源管理和作业调度
>
> Hive：Hadoop的数据仓库，也是sql引擎，为企业决策支持
>
> HBase：采用HDFS为文件存储系统，采用列式存储，可以存储半结构化和非结构化的数据
>
> ElasticSearch：开源的分布式全文检索引擎，也是首选的

#### 大数据产业生态

数据分析与人工智能：SAS SPSS AzureML PAI（阿里） Sophon（星环）

大数据基础平台：CDH HDP MapR FI（华为） TDH（星环）

数据中心操作系统（Docker）：DC/OS Kubernetes TOS（星环）

> 大数据基础平台领域，国内的技术水平并不落后，但是创新能力需要加强
>
> 容器技术的使用，使得大数据云计算化，云计算向大数据渗透（走向融合）

### 分布式数据存储HDFS

应用场景：海量数据的数据分析，批处理

#### HDFS简介

什么是HDFS（设计理念）

Hadoop分布式文件系统，是GFS的开源实现，是Apache Hadoop的核心子项目，在开源大数据技术体系中地位无可替代。

- 运行在廉价的商用机器上，在硬件错误是常态的情况下，具备高容错性
- 一次写入多次读取，支持追加，不支持覆盖
- 流式数据访问，批量读而不是随机读写（一旦读起来就不要停）
- 存储大规模数据集（单个文件大，关注横向线性扩展）

优点：

- 高容错，高可用，高扩展
  - 数据冗余多副本，副本丢失之后自动恢复
  - 具备管理节点（NameNode）,相当于企业的CEO，它的高可用保证了集群的高可用
  - 10k节点规模，作为对比，MPP的节点规模在100个左右
- 海量数据存储
- 构建成本低
- 适合大规模离线批处理

缺点：

- 不适合olap（在线分析）
- 不适合存储小文件
- 不支持并发写
- 不支持随机读写（流式读，增量写）

> 分治思想很重要
>
> 为什么不适合小文件：NameNode中保存了所有文件的元数据，所以存小文件很亏，因为大文件和小文件的元数据差不多大。此外，文件比较小的时候，磁盘寻道的时间也会很高。不了解这一点的话，管理节点很快就会满，但是整个文件系统还有很多浪费的地方

#### HDFS原理

> 先讲架构，再讲存储，再讲读写机制，再讲高可用性

**架构**

主节点，NameNode（有两个，一个是活跃的主节点，另一个是热备主节点，即备份）。主节点管理者所有文件的元数据，如果NameNode（包括活跃的和热备的）挂了，那么所有数据都无法恢复了，即使工作节点上还存着主要的数据。因此主节点需要重点保护。

工作节点，DataNode，负责存储数据，具备心跳机制，每隔一段时间会向主节点做一次汇报，汇报的主要内容是自己是否可用。工作节点中按照块来存储文件，每个块会有副本，并且块和块的副本不能放在同一个数据节点上

客户端，client，负责发命令。

> 例如一个10g文件，HDFS会把数据切分成128M大小的块（这个大小不是随便定的，以后的计算分析都要用到，牵一发动全身），一共80个块。每一个块会被复制成三份，存到不同的datanode中间。工作节点并不会保存自己的块是哪几个，也不知道自己种的块和那些其他节点中的块属于同一个文件，这个表会被保存在NameNode中。即NameNode中会保存以下内容：
>
> file1: block1,block2,...,blockn
>
> block1: node1,node4,node78
>
> ...
>
> blockn: node3.node7,node67

Active NameNode(AN)：理论上不会出现两个active的的主节点，但是由于分布式系统的cap理论，可能会出现脑裂现象，即出现两个活动主节点。（其中一个主节点对外接口失效，与另一个主节点失联，这时候另一个主节点会自动激活）。

- 活动主节点需要管理元数据
- 自动分配和调整块以及块的副本
- 需要处理客户读写要求

> 强管理

StandBy NameNode(SN)：热备的主节点

- Hadoop3.0允许多个备用主节点
- 活动主节点挂了之后，快速升级为活动主节点
- 定期同步编辑日志，定期合并内存镜像和编辑日志到本地磁盘

> 关系数据库的热备就是这么干的，思路差不多，但是大数据这边要复杂很多

DataNode(DN)：工作节点，数量可以很大

- 可以大规模扩展
- 存储大文件的各个块以及校验和
- 执行读写
- 通过心跳与NameNode汇报
- 集群启动时，工作节点向主节点提供块列表信息
- 只负责存储和读写，不负责保存块之间的逻辑信息

Block：数据块

- 文件存储的最小单元
- 大小固定，可以（最好不要）自定义
- 如果一个块比较小，不会占用整个块空间（可伸缩）
- 默认情况下是3副本

Client：客户端

- 切分文件到小块
- 下发读写命令
- 获取文件元数据
- 管理HDFS

**存储**

数据的存储

- block大小不能太大不能太小
- 自动的机架感知和负载均衡：块的副本放置在不同的机架上（机架信息是初始化事输入的）

> 副本1：放在client所在节点，远程client的时候随机选择节点
>
> 副本2：不同的机架上
>
> 副本3：放在第二个副本上同一机架的负载比较小的节点上
>
> 节点选择：同等条件下优先选择空闲节点

元数据的存储

- 内存元数据（内存中）
- 文件元数据（硬盘中）
- 编辑日志：client请求的时候，请求会县写入日志，再进入内存
- 元数据镜像检查点文件：定时更新，用于磁盘更新

> 编辑日志和元数据镜像检查文件如何合并？（如何生成镜像文件）
>
> hadoop1.x
>
> 1. 热备节点下载编辑日志和镜像文件
> 2. 在镜像文件恢复到内存之后，执行一个编辑日志中的动作
> 3. 执行完成之后形成新的镜像文件，传回给主节点
>
> hadoop2.x
>
> 1. 在高可用集群JournalNode并行写入编辑日志和镜像文件
> 2. 热备节点从高可用集群中下载并执行
> 3. 执行完成之后传回给主节点

**写操作**

1. 客户端请求上传文件
2. 主节点检查目录和权限
3. 主节点允许客户端上传，返回批文
4. 客户端切分文件
5. 客户端上传文件
6. 主节点根据块放置策略生成方案
7. 返回生成方案给客户端
8. 客户端拿着批文和放置方案与数据节点建立块传输管道
9. 以块为单位发送数据
10. 发送完成之后再发送一个请求给主节点

> 先写数据再写编辑日志

**读操作**

1. 客户端请求读数据
2. 主节点查询目录
3. 主节点返回块列表
4. 客户端按照列表一个一个访问
5. 客户端拼接块成为文件

**安全模式**

安全模式是HDFS的一种特殊状态，只能读不能写。进入安全模式的主要指标是心跳，即数据节点汇报的自己可用的块的数量。主节点会比较上次总的可用块和这次的可用块，如果这个比值低于99.9%，主节点会立即进入安全模式进行自我修复，从副本中恢复快。直到这个比值高于99.9%，主节点才会离开安全模式。

> 不推荐强制退出特殊状态

触发安全模式的原因：

- 主节点重启（忌讳重启服务器）
- 主节点磁盘空间不足
- 块上报率低于99.9%
- 数据节点无法正常启动
- 日志中出现严重异常
- 用户操作异常

**高可用**

核心是主节点的高可用，核心的核心是元数据的高可用

使用QJM机制保证元数据高可用

- 由于分布式系统的不稳定（cap理论），我们需要在一致性（C）上牺牲一下，并保持最终一致性。
- 部署奇数个JournalNode，它负责存储操作日志
- 在写操作日志的时候，只要超过半数的的JournalNode返回成功，那就认为是成功的

利用ZooKeeper实现Active节点选举

> ZooKeeper是实现关键业务的非常好的系统。
>
> 选举系统：znode树数据结构（持久和临时），活动主机点和znode树之间一个临时节点有一个对应的关系，保持心跳联系。当zookeeper等不到心跳信息的时候，临时节点自动注销，然后zookeeper会发送请求到其余所有热备主节点，热备主节点之间投票选举之后，选举称为新的活动主节点之后，zookeeper会任命它是活动主节点，其余的是热备主节点。

#### HDFS文件管理

**shell命令**

命令用`hadoop fs <args>`或者`hdfs dfs <args>`，其余和shell差不多

**REST API**

写文件：提交一个put请求来做一些设定和前置条件，再提交另一个put来写文件

读文件用get请求

删除文件用delete请求

#### HDFS系统管理

核心配置文件：core-site.xml

HDFS局部配置：hdfs-site.xml

环境变量文件：Hadoop-env.sh

管理常用命令

`hdfs namenode [-format [-clustered cid][-force] [-nonInteractive] ] | [-recover [-force] ]`主节点格式化或回复

`hdfs dfsadmin [generic_options] [-report [-live] [-dead] [-decommissioning] ]`报告文件系统信息

` hdfs fsck <path> [-move | -delete] | [-files [-blocks [-locations | -racks] ] ]` 检查文件系统健康状态

`Safemode`安全模式的进入与离开

`NameNode HA`主备切换

` hdfs dfsadmin [generic_options] -refreshNodes`数据节点的退役和服役

> 1. 先把要退役的数据节点加入黑名单
> 2. 刷新，数据迁移
> 3. 迁移完成之后白名单去除（不再加入新的数据）
> 4. 刷新
> 5. 可以删除节点

`Balancer & BalancerBandwidth`数据重分布

`Distcp` 分布式拷贝

`Snapshot`快照

`Quota` 配额限制

> 实验占一部分，笔试占一部分
>
> 4人1组，只有19个集群，所以要分4大组做实验
>
> 参考书：Hadoop导论
>
> 第一次实验是下下周

### 分布式计算框架

这两者联系很紧密，并且MapReduce并没有被完全弃用，所以要学习MapReduce

#### MapReduce

是一个计算模型，采用的思路是分而治之，并行计算，这个过程中数据不动，计算在移动。

> 把MapReduce程序写好之后，管理节点会分解其为一个一个的任务，任务会通过网络分配给对应数据的数据节点。因此这是计算在移动，而数据不动。

问题在于，这个模型比较粗糙，分而治之的思想比较简单。

特点：计算跟着数据走，良好的扩展性，高容错，提供状态监控，适合海量数据的离线批处理，降低了分布式变成的门槛

使用场景：数据统计，搜索引擎构建索引，海量数据查询，复杂数据分析算法实现

不使用场景：OLAP，流计算，DAG计算

原理：切片（用作映射部分的输入）-映射（切几份就有几个映射任务）-洗牌（Map的输出转换成Reduce的输入）-规约（整合数据）-结果

作业和任务

- 作业：大的，有明确目标的工作单元
- 任务：将作业分解后得到的细分工作单元

切片的大小默认等于block大小（没有严格的要求），每个切片交给一个map任务处理，切片的大小大了话，每个计算节点任务加重，但是总的切片数量变少，切片越小，负载越均衡，但集群的开销越大

- Map阶段(映射)
  - 由若干Map任务组成，任务数量由Split数量决定
  - 输入:Split切片(key-value)，输出:中间计算结果(key-value) 
- Reduce阶段(化简)
     - 由若干Reduce任务组成，任务数量由程序指定
     - 输入:Map阶段输出的中间结果(key-value)，输出:最终结果(key-value) 
- Shuffle阶段(洗牌)
     - 中间环节，负责分区，排序，溢写，合并，抓取
     - 这个环节决定了映射任务输出的每条数据在哪个分区，交给哪个化简任务处理
     - Reduce任务的数量决定了Partition数量 
     - Partition编号 = Reduce任务编号 =“key hashcode % reduce task number” 
     - 避免和减少Shuffle是MapReduce程序调优的重点 
     - Reduce拿到的数据差不多，并且是随机的

作业运行模式：

- JobTracker/TaskTracker模式：Hadoop1
- YARN模式：Hadoop 2

> 大数据有两个场景
>
> 应用场景
>
> 业务场景：数据仓库（数据的汇总），数据集市（解决具体业务问题），数据流动，综合搜索，关系分析，机器学习

#### Spark

- Spark Core
- Spark SQL
- Spark Streaming
- Spark MLib
- Spark GraphX

特点

- 计算高效：主要使用内存，使用DAG引擎，利用多线程池模型
- 通用易用：适用于几乎所有场景，提供了很多api
- 运行模式多样：可以单独使用，也可以YARN/Mesos模式

编程模型：弹性分布数据集RDD

- 分布在集群中的只读对象集合，
- 由多个Partition组成
- 通过转换操作构造 
- 失效后自动重构(弹性) 
- 存储在内存或磁盘中 

RDD操作包括

- Transformation（转化）
  - 将Scala集合或Hadoop输入数据构造成一个新RDD，通过已有的RDD产生新RDD
  - 惰性执行:只记录转换关系，不触发计算
- Action（动作）
  - 转化的实施的地方
  - 真正触发计算

速度比MapReduce提高1到2个数量级

### 分布式SQL引擎Incepter

Hive：数据仓库技术的鼻祖，它解决的问题是一个很重要的数据分析问题，例如银行的交易数据，存在oracle数据库中，容量瓶颈是100G，100G以外的数据放在日志文件（文本文件）中，所以要对这个文本文件进行分析。这个文本文件是有结构的，但是没有写在里面，可以把这个文件放到HDFS里面分析。Hive做的事情就是搞了一个SQL引擎，将SQL语句转化成MapReduce程序。即Hive使用SQL语句和metadata自动转化成MapReduce程序。

Incepter在技术上，底层是Hive+Spark。其定位是一个通用的SQL引擎，用于离线分析和交互式分析。

特点：完整支持SQL语法与存储过程（95%），支持Oracle，DB2，Teradata方言，帮助用户低成本迁移传统应用。支持完整分布式事务处理。右移的大数据处理和分析性能。提供便捷的SQL开发调试工具Waterdrop。

> 大数据是一个读时系统，即不管存什么都可以存，但是读出来的时候要校验（非常宽松）
>
> 关系型数据库是一个写时系统，即写入读取都需要校验各种格式和关系

数据模型，从上到下依次是

- Database 概念上等同于关系型数据库的数据库，但在落地时，系统会为每个数据库创建一个目录，目录名字是数据库名.db
- Table 等同于关系型数据库的数据表，但是不存关系型数据库里的主键，依赖等。
  - 元数据存在Metastone中，实际数据存在HDFS里面，缺省时在数据库的目录下。
  - 按照所有权分类：内表是系统具有完全控制权的表，系统可以删除内表，删除内表时，会先删除元数据，接着删除真实数据。外表是系统没有完全控制权的表，表的元数据存在Metastone中，真实数据在外部存储目录中，删除的时候只会删除元数据，不会删除真实数据（也就是说只是这个系统用不了了，其他系统还能用）。通常来说要建立外表，一是为了安全，另一个是为了给其他系统使用。
  - 按照存储格式分类：Text表是系统默认的表类型，无压缩，航存储，仅支持批量Insert，性能比较低，主要用于导入初始文本建立过渡表。ORC事务表是由ORC表衍生出来的表，继承了ORC表的所有特性，支持完整CURD，以及分布式事务管理。Holodesk表是做数据集市的cube的，支持列式存储，在内存和闪存中，支持在线分析和查询。HyperBase表是数据存在HBase上的表。
- Bucker（分桶）：为了提高select时的join效率问题。采用哈希取模的方式，按照某个字段将表或分区中的数据随机均匀分发到N个桶中，N一般是质数，桶编号是0，1，2，……N-1。如果这个字段是两个表共有的，那另一个表中也分好了桶（两个分桶必须是倍数或者相等关系），这样只需要把每个桶join，再合并之后就可以了。
- Partition（分区）：为了解决select语句中的where语句。将一个大的数据表按照某些字段分解成几个表，这些字段要满足的条件是离散字段，高频字段，平衡字段，常用的分区键例如时间（某一月），籍贯（某个省）等。分区键不再作为数据条目的字段。（不出现在schema中）

> 通常是先分区后分桶
>
> 分桶的键和分桶的数目在剪标确定，不允许更改。

- File：底层的文件

大数据环境下的读时模式：写的时候不校验，读的时候校验宽松，不满足schema的时候也可以读出来（会做处理）